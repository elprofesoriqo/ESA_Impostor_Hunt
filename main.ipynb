{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake Text Detection - Training and Analysis\n",
    "\n",
    "https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport logging\nimport warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import learning_curve\n\nfrom detector import FakeTextDetector\nfrom utils import DatasetPaths\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\nlogger = logging.getLogger(__name__)\n\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\nprint(\"Setup complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Basic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.getcwd()\n",
    "paths = DatasetPaths(\n",
    "    train_dir=os.path.join(base_dir, \"train\"),\n",
    "    test_dir=os.path.join(base_dir, \"test\"),\n",
    "    train_csv_path=os.path.join(base_dir, \"train.csv\")\n",
    ")\n",
    "\n",
    "if not all(os.path.exists(p) for p in [paths.train_dir, paths.test_dir, paths.train_csv_path]):\n",
    "    print(\"DS not found.\")\n",
    "    \n",
    "df_train = pd.read_csv(paths.train_csv_path)\n",
    "print(f\"Training data shape: {df_train.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data Analysis\")\n",
    "print(f\"Total samples: {len(df_train)}\")\n",
    "print(f\"Unique article IDs: {df_train['id'].nunique()}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "class_counts = df_train['real_text_id'].value_counts().sort_index()\n",
    "print(class_counts)\n",
    "\n",
    "class_1_ratio = class_counts[1] / len(df_train)\n",
    "class_2_ratio = class_counts[2] / len(df_train)\n",
    "print(f\"\\nClass 1 (real_text_id=1): {class_1_ratio:.1%}\")\n",
    "print(f\"Class 2 (real_text_id=2): {class_2_ratio:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "class_counts.plot(kind='bar', ax=ax1, color=['skyblue', 'lightcoral'])\n",
    "ax1.set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Real Text ID')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "ax2.pie(class_counts.values, labels=[f'Class {i}' for i in class_counts.index], \n",
    "        autopct='%1.1f%%', colors=['skyblue', 'lightcoral'])\n",
    "ax2.set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset is {'balanced' if abs(class_1_ratio - 0.5) < 0.1 else 'imbalanced'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from utils import read_text_pair\n\ndetector = FakeTextDetector()\n\nsample_stats = []\nsample_texts = []\n\nfor i, row in df_train.head(5).iterrows():\n    article_id = int(row['id'])\n    real_text_id = row['real_text_id']\n    \n    text1, text2 = read_text_pair(paths.train_dir, article_id)\n    \n    real_text = text1 if real_text_id == 1 else text2\n    fake_text = text2 if real_text_id == 1 else text1\n    \n    sample_texts.append({\n        'article_id': article_id,\n        'real_text': real_text[:200] + '...' if len(real_text) > 200 else real_text,\n        'fake_text': fake_text[:200] + '...' if len(fake_text) > 200 else fake_text\n    })\n    \n    sample_stats.append({\n        'article_id': article_id,\n        'real_text_length': len(real_text),\n        'fake_text_length': len(fake_text),\n        'real_word_count': len(real_text.split()),\n        'fake_word_count': len(fake_text.split()),\n        'length_ratio': len(real_text) / len(fake_text) if len(fake_text) > 0 else 0\n    })\n\nsample_df = pd.DataFrame(sample_stats)\nprint(sample_df.to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample Text Pairs\")\n",
    "\n",
    "for i, sample in enumerate(sample_texts[:2]):\n",
    "    print(f\"\\nArticle {sample['article_id']}:\")\n",
    "    print(f\"\\nReal text:\")\n",
    "    print(f\"{sample['real_text']}\")\n",
    "    print(f\"\\nFake text:\")\n",
    "    print(f\"{sample['fake_text']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "all_stats = []\n\nfor i, row in df_train.iterrows():\n    if i % 20 == 0:\n        print(f\"Processed {i}/{len(df_train)} samples\")\n    \n    article_id = int(row['id'])\n    real_text_id = row['real_text_id']\n    \n    text1, text2 = read_text_pair(paths.train_dir, article_id)\n    \n    real_text = text1 if real_text_id == 1 else text2\n    fake_text = text2 if real_text_id == 1 else text1\n    \n    all_stats.append({\n        'article_id': article_id,\n        'real_length': len(real_text),\n        'fake_length': len(fake_text),\n        'real_words': len(real_text.split()),\n        'fake_words': len(fake_text.split()),\n        'length_diff': abs(len(real_text) - len(fake_text)),\n        'word_diff': abs(len(real_text.split()) - len(fake_text.split()))\n    })\n\nall_stats_df = pd.DataFrame(all_stats)\nprint(\"\\nâœ… Analysis complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "axes[0, 0].hist([all_stats_df['real_length'], all_stats_df['fake_length']], \n",
    "                bins=30, alpha=0.7, label=['Real', 'Fake'], color=['blue', 'red'])\n",
    "axes[0, 0].set_title('Character Length Distribution', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Characters')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "axes[0, 1].hist([all_stats_df['real_words'], all_stats_df['fake_words']], \n",
    "                bins=30, alpha=0.7, label=['Real', 'Fake'], color=['blue', 'red'])\n",
    "axes[0, 1].set_title('Word Count Distribution', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Words')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "axes[0, 2].hist(all_stats_df['length_diff'], bins=30, alpha=0.7, color='green')\n",
    "axes[0, 2].set_title('Character Length Difference', fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Absolute Difference')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "\n",
    "axes[1, 0].hist(all_stats_df['word_diff'], bins=30, alpha=0.7, color='orange')\n",
    "axes[1, 0].set_title('Word Count Difference', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Absolute Difference')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1, 1].scatter(all_stats_df['real_length'], all_stats_df['fake_length'], alpha=0.6)\n",
    "axes[1, 1].plot([0, all_stats_df[['real_length', 'fake_length']].max().max()], \n",
    "                [0, all_stats_df[['real_length', 'fake_length']].max().max()], 'r--', alpha=0.8)\n",
    "axes[1, 1].set_title('Real vs Fake Text Length', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Real Text Length')\n",
    "axes[1, 1].set_ylabel('Fake Text Length')\n",
    "\n",
    "data_to_plot = [all_stats_df['real_length'], all_stats_df['fake_length']]\n",
    "axes[1, 2].boxplot(data_to_plot, labels=['Real', 'Fake'])\n",
    "axes[1, 2].set_title('Length Distribution Comparison', fontweight='bold')\n",
    "axes[1, 2].set_ylabel('Characters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Summary Statistics\")\n",
    "print(f\"Real text - Mean length: {all_stats_df['real_length'].mean():.0f}, Std: {all_stats_df['real_length'].std():.0f}\")\n",
    "print(f\"Fake text - Mean length: {all_stats_df['fake_length'].mean():.0f}, Std: {all_stats_df['fake_length'].std():.0f}\")\n",
    "print(f\"Mean length difference: {all_stats_df['length_diff'].mean():.0f}\")\n",
    "print(f\"Mean word difference: {all_stats_df['word_diff'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Fake Text Detector\")\n",
    "\n",
    "detector = FakeTextDetector(\n",
    "    n_splits=7,\n",
    "    n_repeats=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Detector initialized with:\")\n",
    "print(f\"   - Cross-validation folds: {detector.n_splits}\")\n",
    "print(f\"   - Repetitions: {detector.n_repeats}\")\n",
    "print(f\"   - Total training runs: {detector.n_splits * detector.n_repeats}\")\n",
    "print(f\"   - Random seed: {detector.seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df, cv_scores = detector.train_and_predict(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cv_scores and any(cv_scores.values()):\n",
    "    metrics_to_plot = [metric for metric, scores in cv_scores.items() if scores]\n",
    "    \n",
    "    if metrics_to_plot:\n",
    "        fig, axes = plt.subplots(1, len(metrics_to_plot), figsize=(5*len(metrics_to_plot), 5))\n",
    "        \n",
    "        if len(metrics_to_plot) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            scores = cv_scores[metric]\n",
    "            \n",
    "            # Box plot\n",
    "            axes[i].boxplot([scores], labels=[metric.upper()])\n",
    "            axes[i].set_title(f'{metric.upper()} Distribution', fontweight='bold')\n",
    "            axes[i].set_ylabel('Score')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add mean line\n",
    "            mean_score = np.mean(scores)\n",
    "            axes[i].axhline(y=mean_score, color='red', linestyle='--', alpha=0.7, \n",
    "                           label=f'Mean: {mean_score:.4f}')\n",
    "            axes[i].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No cross-validation scores available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Submission shape: {submission_df.shape}\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "pred_counts = submission_df['real_text_id'].value_counts().sort_index()\n",
    "print(pred_counts)\n",
    "\n",
    "pred_ratio_1 = pred_counts[1] / len(submission_df) if 1 in pred_counts else 0\n",
    "pred_ratio_2 = pred_counts[2] / len(submission_df) if 2 in pred_counts else 0\n",
    "\n",
    "print(f\"\\nPrediction ratios:\")\n",
    "print(f\"Class 1 predictions: {pred_ratio_1:.1%}\")\n",
    "print(f\"Class 2 predictions: {pred_ratio_2:.1%}\")\n",
    "\n",
    "balance_diff = abs(pred_ratio_1 - 0.5)\n",
    "if balance_diff < 0.1:\n",
    "    balance_status = \"Well balanced\"\n",
    "elif balance_diff < 0.2:\n",
    "    balance_status = \"Slightly imbalanced\"\n",
    "else:\n",
    "    balance_status = \"Highly imbalanced\"\n",
    "\n",
    "print(f\"Balance status: {balance_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "pred_counts.plot(kind='bar', ax=ax1, color=['lightblue', 'lightgreen'])\n",
    "ax1.set_title('Test Set Predictions', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Predicted Real Text ID')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "ax2.pie(pred_counts.values, labels=[f'Pred {i}' for i in pred_counts.index], \n",
    "        autopct='%1.1f%%', colors=['lightblue', 'lightgreen'])\n",
    "ax2.set_title('Test Predictions Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "train_ratios = df_train['real_text_id'].value_counts().sort_index() / len(df_train)\n",
    "train_ratios.plot(kind='bar', ax=ax1, color=['skyblue', 'lightcoral'], alpha=0.8)\n",
    "ax1.set_title('Training Set Distribution', fontweight='bold')\n",
    "ax1.set_ylabel('Ratio')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "pred_ratios = submission_df['real_text_id'].value_counts().sort_index() / len(submission_df)\n",
    "pred_ratios.plot(kind='bar', ax=ax2, color=['lightblue', 'lightgreen'], alpha=0.8)\n",
    "ax2.set_title('Test Predictions Distribution', fontweight='bold')\n",
    "ax2.set_ylabel('Ratio')\n",
    "ax2.tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Distribution Comparison:\")\n",
    "print(f\"Training - Class 1: {train_ratios[1]:.1%}, Class 2: {train_ratios[2]:.1%}\")\n",
    "print(f\"Predictions - Class 1: {pred_ratios[1]:.1%}, Class 2: {pred_ratios[2]:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(base_dir, \"submission.csv\")\n",
    "submission_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {output_path}\")\n",
    "print(f\"Submission preview:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "if 'accuracy' in cv_scores and cv_scores['accuracy']:\n",
    "    print(f\"ðŸŽ¯ Final CV Accuracy: {np.mean(cv_scores['accuracy']):.1%}\")\n",
    "print(f\"Class balance: {pred_ratio_1:.1%} vs {pred_ratio_2:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}